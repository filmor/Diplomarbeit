\section{Proof of Watson's Lemma}
\label{sec:proof-watson}
\begin{Theorem}
  \input{snippets/watsons-lemma}
  \begin{Proof}
    The proof due to \cite{Miller2006} uses the Taylor series expansion of
    $g(t)$ with a remainder term in the integrand.

    Since we only assume differentiability in some neighbourhood of $t=0$ we
    need to estimate the integral for some $0 < s < T$, with $s$ being
    sufficiently small such that $g\in C^{\infty}[0,s]$ is analytic. We
    can now split up the integral $F(x)$ as
    \begin{equation*}
      F(x) = \Integ[s]{0}{t}{\Eto{-xt}\phi(t)} +
      \Integ[T]{s}{t}{\Eto{-xt}\phi(t)}.
    \end{equation*}

    The integral over $\left[s,T\right]$ can be easily estimated as
    \begin{equation*}
      \Abs{\Integ[T]{s}{t}{\Eto{-xt}\phi(t)}}
      \le \Eto{-xs} \Integ[T]{s}{t}{\Abs{\phi(t)}}
      \le \Eto{-xs}\Integ[T]{0}{t}{\Abs{\phi(t)}}.
    \end{equation*}
    Since we assumed this integral to be finite and $s>0$ we have for
    $x\to\infty$
    \begin{equation*}
      \Integ[T]st{\Eto{-xt}\phi(t)} = O(x^{-\infty}),
    \end{equation*}
    so we see that this term doesn't contribute to the asymptotic expansion.
    For the first term, the integral over $[0,s]$ we use the taylor expansion of
    $g$ with remainder
    \begin{equation*}
      g(t) = \sum_{n=0}^{N} \frac{g^{(n)}(0)}{n!}t^n + r_N(t).
    \end{equation*}
    We thus have
    \begin{equation*}
      \Integ[s]0t{\Eto{-xt}t^\sigma g(t)} = \sum_{n=0}^N
      \frac{g^{(n)}(0)}{n!} \Integ[s]0t{\Eto{-xt}t^{\sigma+n}} +
      \Integ[s]0t{\Eto{-xt}t^\sigma r_N(t)},
    \end{equation*}
    where we can estimate the remainder term as
    \begin{align*}
      \left|\Integ[s]0t{\Eto{-xt}t^\sigma r_N(t)}\right| &\le
      \Integ[s]0t{\Eto{-xt}t^\sigma\left|r_N(t)\right|} \\
      &\le \sup_{\tau\in\left[0,s\right]}
      \frac{\bigl|g^{(N+1)}(\tau)\bigr|}{(N+1)!}\Integ[s]0t{\Eto{-xt}t^{\sigma+N+1}}.
    \end{align*}
    It thus suffices to estimate the asymptotic behaviour of integrals of
    the type $F_\alpha(x) := \Integ[s]0t{\Eto{-xt}t^\alpha}$ for
    $x\to\infty$. We have
    \begin{align*}
      F_\alpha(x) &= \Integ[\infty]{0}{t}{\Eto{-xt}t^\alpha} -
      \Integ[\infty]{s}{t}{\Eto{-xt}t^\alpha} \\
      &= x^{-(\alpha+1)}\Integ[\infty]{s}{\tau}{\Eto{-\tau}\tau^\alpha} -
      \Integ[\infty]{s}{t}{\Eto{-xt}t^\alpha} \\
      &= x^{-(\alpha+1)}\Gamma(\alpha + 1) - 
      \Integ[\infty]{s}{t}{\Eto{-xt}t^\alpha}.
    \end{align*}
    For the second term we can use the HÃ¶lder equation in the special case
    $p=q=2$, where we split the exponential as $\Eto{-xt} =
    \Eto{-xt/2}\Eto{-xt/2}$, to get
    \begin{equation*}
      \Integ[\infty]{s}{t}{\Eto{-xt}t^\alpha} \le
      \sqrt{\Integ[\infty]{s}{t}{\Eto{-xt}}} \cdot
      \sqrt{\Integ[\infty]{s}{t}{\Eto{-xt}t^{2\alpha}}}
      = \Eto{-xs/2} \sqrt{\Integ[\infty]{s}{t}{\Eto{-xt}t^{2\alpha}}}.
    \end{equation*}
    The second term in the product decreases with $x$, and since we are
    interested in the case $x\to\infty$ we can assume $x>1$ to estimate the
    square of the second factor as
    \begin{equation*}
      \Integ[\infty]{s}{t}{\Eto{-xt}t^{2\alpha}} \underset{x>1}\le
      \Integ[\infty]{s}{t}{\Eto{-t}t^{2\alpha}} =: C_{s,\alpha}
    \end{equation*}
    and thus since $s>0$ we have the following asymptotic behaviour for
    $x\to\infty$:
    \begin{equation*}
      \Integ[\infty]{s}{t}{\Eto{-xt}t^\alpha} \le \Eto{-xs/2}
      \sqrt{\Integ[\infty]{s}{t}{\Eto{-t}t^{2\alpha}}}
      \le C_{s,\alpha}\,\Eto{-xs/2} = O(x^{-\infty}),
    \end{equation*}
    thus this term doesn't contribute to the asymptotic expansion. Using this we
    have
    \begin{equation*}
      F_\alpha(x) = x^{-(\alpha+1)}\,\Gamma(\alpha + 1) + O(x^{-\infty}),
    \end{equation*}
    and inserting $\alpha = \sigma + N + 1$ for the remainder and $\alpha =
    \sigma + n$ for the main term we arrive at
    \begin{align*}
      F(x) &= \sum_{n=0}^N \frac{g^{(n)}(0)}{n!} F_{\sigma+n}
            + F_{\sigma + N + 1} \\
           &= \sum_{n=0}^N \frac{g^{(n)}(0)}{n!}
            \Gamma(\sigma + n +1)\,x^{-(\sigma + n + 1)} + \Gamma(\sigma +
              N + 2)\,x^{-(\sigma + N + 2)} \\
           &= \sum_{n=0}^N \frac{g^{(n)}(0)}{n!}\Gamma(\sigma + n +
            1)\,x^{-(\sigma + n + 1)} + O\bigl(x^{-(\sigma + N + 2)}\bigr).
    \end{align*}
    This proves the statement.
  \end{Proof}
  \begin{Remark}
    A different, more general approach on this is taken in \cite[Ex.~53,
    p.104]{Estrada1993}, where the Lemma is proven by use of the expansion of
    $f(\lambda x)$ for $f(x) = H(x)\Eto{-x}$, $H$ being the Heaviside function.
  \end{Remark}
\end{Theorem}
